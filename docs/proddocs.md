# Accessing AB2D Production Environment

## Audience

At this point, the organization has attested, elected an ADOS, and has received credentials to access production. You are receiving/using this document because you wish to access your organization’s claims data from the production environment. 

## Security Practices

The credential file contains sensitive information granting access to PII/PHI. Do not share this sensitive information in images, in plain text, or in chat.

The most sensitive information provided in this file includes:
- OKTA_CLIENT_ID: unique id of {organization} in the AB2D system
- OKTA_CLIENT_SECRET: unique password granting access to the AB2D system
- AUTH: basic encoding of the client id and password that can be easily decoded

Again be extremely careful distributing this information and DO NOT SEND via email or chat as text, images, or videos.

## Asking for Help

If issues persist using these or other scripts email the AB2D team at [ab2d@semanticbits.com](mailto:ab2d@semanticbits.com) 
or join our Google Groups at: 
[https://groups.google.com/u/1/g/cms-ab2d-api](https://groups.google.com/u/1/g/cms-ab2d-api). 

When emailing the team please include the following information concerning the context of issues:

- What operating system is in use by the machine (Windows or Linux/Mac)?
- What IP address does the machine have (see Verifying Setup)?
- What is the HTTP status code (403, 400, etc.) that is being received, if applicable?
- A description of the issue including the stage of the export causing the issue.
- Any additional logs (be extremely careful with the logs you provide). Refer to the Security Practices section as to which pieces of information are sensitive.

When emailing the team ***do not*** include any form of the credentials provided including any encoded form. Please double check any logs provided to make sure the logs do not contain the organization’s credentials.

## Usage Guide

### API Restrictions:
These export jobs are expensive to run and create files containing sensitive data therefore the API enforces usage limitations and does not allow claims data to persist for too long.

#### List of Restrictions: IMPORTANT
1. Each contract is limited to one running export at a time. Jobs cannot be run concurrently. A job can be cancelled if there is a reason the current job is not needed (or incorrectly submitted) before submitting a new one.
1. ***Files created by an export are only stored by the AB2D API for 72 hours after an export job is completed. If the files are not downloaded 72 hours after creation then those files will be deleted automatically and a new job will have to be created.***
1. ***Files created by an export can only be downloaded once. After being downloaded a file is immediately deleted.***
1. The API can only process so many export jobs at once. If the API is busy a submitted job may be queued for later processing.

What you can do:
- Start a single job, monitor the job, and download within the 72 hour period.
- Only run (1) job at a time. Attempting to start more than one job will fail.
- Cancel a running job whenever you want.

### Intended Usage / Suggestions

Parts A & B claims data are  updated periodically. Pulling data every day or even weekly is unnecessary. Our advice is to pull data bi-weekly, monthly, or quarterly. Additionally, because export jobs are time consuming, especially for large organizations, scheduling export jobs to run overnight is suggested. 
***Be aware that files generated by an export job are only available for 72 hours hours after the job completes.***

The AB2D team supports incremental searches so that a date can be supplied to the search to only pull data updated since a specific date. This will pull data that has changed since a specific date. They may either have been created or updated since that date. The next search can use the latest updated EOB as the since date for the next job or the time of the last search if known.

### AB2D Model

***Explanation of Benefits (EOBs) claims are not exported by the date services were provided but by the date the EOB was fully processed into the CMS system.***
Conducting an export on one date does not guarantee that you have received EOBs for all services provided up to that date, only EOBs processed by that date, as indicated by their updateDate.

##### Services Provided Date vs. Date Processed Scenario

We have two inpatient stays
1. Inpatient Stay on October 1st, 2020
1. Inpatient Stay on October 1st, 2020

For 1, an EOB claim is fully processed into the AB2D system on November 1st, 2020. If an organization exports claims data on November 5th, then claim A will be pulled.

For 2, an EOB claim is fully processed into the AB2D system on December 1st, 2020. If an organization exports claims data on November 5th, the claim for B will obviously not be included. However if the organization later  exports on December 5th, the claim for B will be present and exported.

***EOB data can be added and updated. These updates can occur over a period of time. This can lead to scenarios where an organization exports an EOB twice. Organizations need to de-duplicate and update these EOBs.***

Each EOB claim comes with a unique identifier and a last updated field. These fields can be used to conduct deduplication.

#### Claims Data Update Scenario
1. Claim 123 is added for beneficiary on November 1st 2020
1. Organization XYZ pulls Claim 123 on November 2nd 2020
1. Claim 123 is updated on November 3rd 2020
1. Organization XYZ pulls the updated Claim 123 on November 4th 2020

##### Since Parameter
To reduce the time that jobs take use the _since parameter in API calls. The _since parameter will allow you to grab EOBs data added since a specific date instead of all of the data available to the AB2D API.

##### Example
An example of this would be an organization XYZ that attested on March 1st 2020. The organization runs their first job on November 1st 2020. The organization runs their second job on December 1st 2020.

If XYZ does not use the _since parameter on their second job:

1. The first job takes four hours to export all data available to them added between March 1st, 2020 and November 1st 2020.
2. The second job takes four and a half hours to export all data available to them added between March 1st 2020 and December 1st 2020.

If XYZ uses the _since parameter and sets _since equal to November 1st 2020 on their second job:

1. The first job takes 4 hours to export all data available to them between March 1st, 2020 and November 1st 2020.
2. The second job takes half an hour to only export claims data available to them between November 1st, 2020 and December 2020.

### Expected Workflow

A job can be broken down into four phases. These phases will be standard no matter the programming language or platform used to interact with the AB2D API. Obtaining a token is done against a third party identification provider.

| Step | Goal | Swagger | Time | Frequency |
| --- | ---- | ----- | --- | --- |
| Obtain a JSON Web Token | Get a token for authentication with the AB2D API | N/A | Seconds | At least every hour during the job |
| Start a Job | Start a job and save the unique id of the job | Export API | Seconds | Once a job |
| Monitor a Job | Wait for the job to complete and get a list of files to download | Status API | Minutes to hours depending on contract size | Once every few minutes |
| Download the Files | Download the files listed in the returned data from the Status API from a complete job | Download API | Minutes to hours depending on Internet download speeds | Once a job

## Quickstart Step by Step Guides

This guide is meant to provide a guide for an organization’s ***first production run***. As part of this guide the AB2D team has built sample scripts demonstrating how to run an export and how to automate an export. These scripts are meant to help organizations get started with the AB2D API but are not meant for long term usage in production as it does not provide sufficient error checking, security or auditing capabilities. Organizations should build up automation for their ETL processes beyond these scripts.

Please focus the most attention on ***Verifying Setup*** and ***Creating the Credentials File*** before attempting any of these scripts. Each individual guide assumes that the aforementioned steps have been completed.

Additionally, read and understand the usage of the Since Parameter as detailed in the Intended Usage / Suggestions section.

### Overview

1. Requirements: information/resources required to export a job
1. Verifying Setup: check that the machine used for exports has the appropriate access and that the credentials provided by the AB2D team work.
1. Creating credentials file: guide to creating a credentials file
1. AB2D Sandbox vs. Production differences that may influence the job.
1. Executing a bulk claim data download
    1. Bash guide (Linux & Mac)
    1. Powershell Guide (Windows)
    1. Python Guide (Any environment but requires Python to be installed)

### Requirements

1. The credentials file provided to your organization’s attestor
1. Access to a machine with a whitelisted IP address
1. Internet access for the whitelisted machine
1. Permission on that machine to download files
1. Permission on that machine to execute a Python script, Bash Terminal or PowerShell Terminal

### Verifying Your Setup
Before attempting to export data confirm the machine’s access to the AB2D API.

#### Verify Your IP Address
Check that the IP  address of the machine performing export is one provided to and whitelisted by the AB2D team.

1. If you have a browser, open it on the machine you are connecting from, and go to this website 
[http://checkip.amazonaws.com/](http://checkip.amazonaws.com/).
Remember your IP should be public and static. Your Organization should have provided no more than 8 IPs to be whitelisted. Should you need to change these, please email us and we will provide you with a form to do so. 
1. If you do not have access to a browser on the connecting machine, query from the command line of the machine you are running on
    1. On Linux/Mac open a terminal and run the following command: “curl -X GET checkip.amazonaws.com”
    1. On Windows open a Powershell terminal and run the following command “Invoke-RestMethod -Method GET checkip.amazonaws.com”

#### Verify AB2D Production Connectivity
Check that the machine performing the export belongs to the provided  IPs .

On the command line run one of the two following commands

- On Linux/Mac in a terminal run:
``` curl -X GET https://api.ab2d.cms.gov/health --verbose ```

- On Windows in a powershell terminal run 
```Invoke-RestMethod -Method GET https://api.ab2d.cms.gov/health```

- In Postman type create a new GET request against the following ```URL: https://api.ab2d.cms.gov/health```.

- In a browser, on the machine, go to the following URL ```https://api.ab2d.cms.gov/swagger-ui/index.html```

If the response has an HTTP status of 200 then the request is okay and your IP address is whitelisted.

If the response has an HTTP status of 403 your URL request has been rejected and your IP address is possibly not whitelisted.

### AB2D Sandbox vs. AB2D Production

- Sandbox: The sandbox AB2D environment is a public environment available for learning how the AB2D API works and to experiment with automation concerning the API.
- Production: The production AB2D environment is a private environment available only to attested and credentialed PDPs for exporting relevant patient data.

Key differences:
- Sandbox - contains test contracts which are publicly available
- URL - The sandbox and production environments have different URLs to access them:
  - Sandbox - ```https://sandbox.ab2d.cms.gov```
  - Production - ```https://api.ab2d.cms.gov```
- Identity provider - the sandbox identity provider is a test provider only which means that you cannot get a bearer token giving you access to real claims data using the sandbox identity provider. Only the production identity provider can give access to your organization’s claims data. The locations from which you can retrieve bearer tokens differ: 
  - Sandbox - ```https://test.idm.idp.cms.gov```
  - Production - ```https://idp.cms.gov```
- IP Whitelisting - our production environment provides extremely limited access. Only IP addresses explicitly provided by your organization can access the production system.

### Create Base64 Credentials Store

[Base64](https://en.wikipedia.org/wiki/Base64) is an encoding type that allows binary data to be encoded into text values that work better with
systems that only reliably support text content. It does not encrypt data, it simply encodes it so that it is easier to transmit.
Because passwords are often unpredictable, most web based authentication requires Base64 encoded credentials. This is the case
with our API. This is also why you must keep both the credentials and the encoded values secret. To receive a JSON Web Token
from Okta to pass to our API, we must pass a Base64 encoded clientId:password to the Basic Authorization header.
You can either create these manually from your credentials or use the one we provided for convenience.
We will be putting this encoded value in a file so that our scripts can use it for authentication. Name the file after
the contract number since authorization is done at the contract level. For this example, let's say the contract
number is `Z123456`, our client ID is `abcd` and password is `badpassword`. If we encoded our example client ID and 
password, it would look like `YWJjZDpiYWRwYXNzd29yZA==`. Your credentials will likely be a little longer, but that
will give you an idea of what the value looks like.

#### Use the One Supplied in Credential File

For your convenience, we have provided the Base64 encoded clientId:password. 

1. Open the credentials file provided by the AB2D team
1. Copy the Base64 encoded string inside the credential file
1. Create a new text file using your editor of choice
1. Paste the Base64 encoded string into the new text file (the file should only have one line and look similar to our example)
1. Save that file as the credential file (for example `C:\users\abcduser\credentials_Z123456_base64.txt`) and note 
the location and name of the file for later as it will be a parameter in other scripts.

If you wish to manually create this file from your credentials, you can:

#### In Linux or Mac:
You will be creating a file in a directory to place the Base64 credentials. Make sure you have write access to that
directory. Let's use `credentials_Z123456_base64.txt` in the user's home directory of `/home/abcduser` as an example.

Open a bash terminal and type:

```
AUTH_FILE=/home/abcduser/credentials_Z123456_base64.txt
OKTA_CLIENT_ID=abcd
OKTA_CLIENT_SECRET=badpassword
```
To encode the credentials as base64
```
echo -n "${OKTA_CLIENT_ID}:${OKTA_CLIENT_PASSWORD}" | base64 > $AUTH_FILE
```

#### In Powershell 

Open a PowerShell terminal

Create a new empty file:

```
$AUTH_FILE=C:\users\abcduser\credentials_Z123456_base64.txt
New-Item -Path $AUTH_FILE -ItemType File
```

Create the base64 credentials:
```
$BASE64_ENCODED_ID_PASSWORD='{Base64-encoded abcd:badpassword}'
```
Save the base64 credentials as the only line in the auth file:
```
Set-Content -Path $AUTH_FILE $BASE64_ENCODED_ID_PASSWORD
```

The credential file should contain one line and contain the Base64 encoded value of the client ID and password

## API Guides

This section will describe the different tools you can use to extract bulk data for a contract.

### Bash API Client Step by Step Guide

This guide shows how to use an existing example API client that we provide consisting of several scripts. The example API client is free to use or extend. However, this client is just an example implementation and you should implement your own client suited to your needs.

| Scripts| |
| :---------------------------------  | --- |
| `bootstrap.sh`        | set up environment variables necessary for other scripts |
| `start-job.sh`        | start a job and save the job id for future use |
| `monitor-job.sh`      | monitor a running job using the saved job id and save list of files to download when the job completes|
| `download-results.sh` | download the created files |

#### Requirements
- Bash shell on the machine with the white listed IP address
- Contract number if there are more than one
- Directory - the directory that you wish to save all of the files in
- Auth File - location of the file containing the organization’s base64 encoded credentials
- Since date - if you only want claims data entered or updated in the system after a certain date provide a since date matching
The format of the date is `2020-05-01T00:00:00.000-05:00`
- Enough space on the file system for the exported claims data 

##### Download the Scripts
1. The bash API is available [here](https://github.com/CMSgov/ab2d-sample-client-bash). Either download a ZIP file of
the repository or clone the repository using these [instructions](https://docs.github.com/en/free-pro-team@latest/github/creating-cloning-and-archiving-repositories/cloning-a-repository).

1. Unzip or move the files into a specified directory. For our example, let's create a directory called `ab2d` in the home directory.
Copy those script files to that directory (`/home/abcduser/ab2d`).

##### Run the Scripts

###### Setup the Environment
1. Open a bash shell. Use this same shell to run all commands. We will be setting environment variables that are only
valid inside this shell. ***Do not close this terminal before the download is complete***.
1. Go to the directory where the script files are located (in our example `/home/abcduser/ab2d`). Perform a `ls` in that directory to make sure the four scripts are there
1. Remember where you put the Base64 credential file (in our example, it was `/home/abcduser/credentials_Z123456_base64.txt`).
1. Run the following command:
```
source bootstrap.sh -prod --directory /home/abcduser/ab2d --auth /home/abcduser/credentials_Z123456_base64.txt --since 2020-05-01T00:00:00.000-05:00
```
Verify that the command worked and defined the correct environment variables:
```
echo $DIRECTORY
/home/abcduser/ab2d

echo $IDP_URL
https://idm.cms.gov/oauth2/aus2ytanytjdaF9cr297/v1/token

echo $API_URL
https://api.ab2d.cms.gov/api/v1/fhir

echo $AUTH_FILE
/home/abcduser/credentials_Z123456_base64.txt

echo $SINCE
2020-05-01T00:00:00.000-05:00
```

###### Start a Job
This script will use the $export endpoint to start a job. From the same window used to create the environment variables and in the same directory where the scripts are located:
```
./start-job.sh
```
Verify that a file called `jobId.txt` was created. 

###### Monitoring a Job
Because this job is asynchronous, you must check on the status of the job periodically until the job is complete.
When a job completes, the response to the status check will be the created files. This script will 
continue to check the status until it is complete. You do not have to run this script more than once (unless you exit it)
as it will pause and recheck. Use the same shell.

```
./monitor-job.sh
```

Once this script exists, it should have created a file called `response.json` in the current directory.
This file will contain all the names of the files created as part of this batch job. Files have a max size
 so if the number of claim data records causes a file to exceed that size, a new file is created.
 
###### Download the Claims Data Files
Once `monitor-job.sh` finishes, we can now download the files from AB2D (again from the same shell)

```
./download-results.sh
```
The files will be NDJSON (New line delimited JSON) files. That means each line in the files is one claim written in JSON.
The naming standard for the files is to use the contract number and then a number indicating which value it is in the series
of created files.

In the directory you specified as the parameter to the `bootstrap.sh` command (in our case 
`/home/abcdhome/ab2d`), you can see the files. If there are two files for our sample contract `Z123456`, you should see:
```
ls /home/abcdhome/ab2d/*.ndjson
Z123456_0001.ndjson
Z123456_0002.ndjson
```
### Windows API Client Step by Step Guide
The Windows API Client will always download files to your current working directory. This means that 
if you run a job you should move the files out of the directory afterwards into a new directory 
just for that download.

#### Requirements
- Powershell on the machine with the white listed IP address
- Contract number if there are more than one
- Directory - the directory that you wish to save all of the files in
- Auth File - location of the file containing the organization’s base64 encoded credentials
- Since date - if you only want claims data entered or updated in the system after a certain date provide a since date matching
The format of the date is `2020-05-01T00:00:00.000-05:00`
- Enough space on the file system for the exported claims data 

#### Download API Client
1. Download the bash api client available [here](https://github.com/CMSgov/ab2d-sample-client-powershell). 
Either download a ZIP file of the repository or clone the repository using these [instructions](https://docs.github.com/en/free-pro-team@latest/github/creating-cloning-and-archiving-repositories/cloning-a-repository).
2. Open powershell terminal and navigate to the home directory of the project
3. Run the `dir` command. You should see two powershell scripts (.ps1) and a `README`

#### Prepare the Environment Variables
1. Open a PowerShell terminal and go do the directory which contains the powershell scripts.
1. Set the location of the file containing your organizations AB2D credentials encoded as Base64
    ```
    $AUTH_FILE=C:\users\abcduser\credentials_Z123456_base64.txt
    ```

1. Set the URL that the authentication provider is available at 
    ```
    $AUTHENTICATION_URL='https://idm.cms.gov/oauth2/aus2ytanytjdaF9cr297/v1/token'
    ```

1. Set the URL that AB2D is available at
    ```
    $AB2D_API_URL='https://api.ab2d.cms.gov/api'
    ```

#### Start and Monitor a Job
This script will use the environment defined above, create a new job and monitor it
until it is complete. Once complete, it outputs a list of files to be downloaded

1. In the same PowerShell terminal start and monitor the job:
    ```
    $JOB_RESULTS = &.\create-and-monitor-export-job.ps1 | select -Last 1
    ```
1. Check the contents of the variable `JOB_RESULTS`. It should contain the list of files to download.
    ```
    $JOB_RESULTS to check the contents of the variable
    ```
    Do not close the shell so that the value of `JOB_RESULTS` is preserved.
    
#### Download file(s)
Next, we download the files specified in `JOB_RESULTS` into the current directory

1. In the same PowerShell terminal download the results
    ```
    .\download-results.ps1
    ```
2. Check whether the files were downloaded
    ```
    dir
    ```
   
   The files will be NDJSON (New line delimited JSON) files. That means each line in the files is one claim written in JSON.
   The naming standard for the files is to use the contract number and then a number indicating which value it is in the series
   of created files.
       
   For example, if there are two files for our sample contract `Z123456`, you should see:
   ```
   Z123456_0001.ndjson
   Z123456_0002.ndjson
   ```
   
### Python API Client Step by Step Guide
The Python API Client can be used in any environment, but must be installed on the machine with the white listed IP address.
To install Python3 and Pip (Python Installation Installer), go [here](https://realpython.com/installing-python/).
Make sure you have at least Python 3.6 and pip3 installed. Some machines come with Python2 by default which
is deprecated.

#### Requirements
Bash or PowerShell
- Python3 and Pip installed
- Contract number if there are more than one
- Directory - the directory that you wish to save all of the files in
- Auth File - location of the file containing the organization’s base64 encoded credentials
- Since date - if you only want claims data entered or updated in the system after a certain date provide a since date matching
The format of the date is `2020-05-01T00:00:00.000-05:00`
- Enough space on the file system for the exported claims data 

#### Download API Client
1. Download the bash api client available [here](https://github.com/CMSgov/ab2d-sample-client-python). 
Either download a ZIP file of the repository or clone the repository using these [instructions](https://docs.github.com/en/free-pro-team@latest/github/creating-cloning-and-archiving-repositories/cloning-a-repository).
1. Open a terminal and navigate to the home directory of the downloaded files. Let's say it is either
`C:\Users\abcduser\ab2d\` for windows or `/home/abcduser/ab2d` for linux/Mac. Verify the files
exist by either doing a `dir` or `ls`. You should see the `job-cli.py` script and a `README`

#### Prepare Environment Variables

1. In a Bash Shell or PowerShell terminal, navigate to the directory with the script file.
1. Set the variable `AUTH_FILE` which points to an existing file containing the base64 credentials

    | Environment | Command |
    | --- | --- |
    | Linux/Mac: | `AUTH_FILE=/home/abcduser/credentials_Z123456_base64.txt` |
    | Windows: | `$AUTH_FILE=C:\users\abcduser\credentials_Z123456_base64.txt` |

1. Define a directory to save the exported files in. Create it if necessary. This is where the export files
will be saved. For our example, let's say it's `/home/abcduser/ab2d` or `C:\users\abcduser\ab2d`.
1. Set the variable `DIRECTORY` which points to that directory in the previous step

    | Environment | Command |
    | --- | --- |
    | Linux/Mac: | `DIRECTORY=/home/abcduser/ab2d` |
    | Windows: | `$DIRECTORY=C:\users\abcduser\ab2d` |
    
1. Do not close this shell

#### Start the Job
Use the same shell used to prepare the environment variables to perform this task. 
1. Make sure the `python` command is mapped to the correct version by performing the following command:
    ```
    python --version
    ```
   You should see a version that is at least 3.6.
1. Start the export job:

    | Environment | Command |
    | --- | --- |
    | Linux/Mac: | `python job-cli.py -prod --auth $AUTH_FILE --directory $DIRECTORY --only_start` |
    | Windows: | `python job-cli.py -prod --auth %AUTH_FILE% --directory %DIRECTORY% --only_start` |
    
1. Verify that a job id was created. The `job_id.txt` file should have been created. The file should contain
a string similar in ***format*** to `133039b8-c74c-422f-8836-8335c13f5a8d`.

    | Environment | Command |
    | --- | --- |
    | Linux/Mac: | `cat $DIRECTORY/job_id.txt` |
    | Windows: | `type %DIRECTORY%\job_id.txt` |

#### Monitor the Job
A job can take anywhere from 2 minutes to several hours. If the monitoring script fails for any
reason, restart the script. There are no side effects from monitoring the job so this script can be run as many times as necessary.
Use the same shell used to prepare the environment variables to perform this task. 

1. Begin monitoring the job

    | Environment | Command |
    | --- | --- |
    | Linux/Mac: | `python job-cli.py -prod --auth $AUTH --directory $DIRECTORY --only_monitor` |
    | Windows: | `python job-cli.py -prod --auth %AUTH% --directory %DIRECTORY% --only_monitor` |
    
1. When the job is completed the script should automatically save a list of files to download. To view the list:

    | Environment | Command |
    | --- | --- |
    | Linux/Mac: | `cat $DIRECTORY/response.json` |
    | Windows: | `type %DIRECTORY%\response.json` |

#### Download the Files

This process will only download the files once. Running again will not overwrite the files but will also not download anything. 
If the download script fails you can run it again without worrying about overwriting files.

1. In the same shell used to prepare the environment variables, download the files:

    | Environment | Command |
    | --- | --- |
    | Linux/Mac: | `python job-cli.py -prod --auth $AUTH --directory $DIRECTORY --only_download` |
    | Windows: | `python job-cli.py -prod --auth %AUTH% --directory %DIRECTORY% --only_download` |

1. List the files that you have downloaded and verify that they are present

    | Environment | Command |
    | --- | --- |
    | Linux/Mac: | `ls $DIRECTORY/*.ndjson` |
    | Windows: | `dir %TARGET_DIR%\*.ndjson` |
    
    The files will be NDJSON (New line delimited JSON) files. That means each line in the files is one claim written in JSON.
    The naming standard for the files is to use the contract number and then a number indicating which value it is in the series
    of created files.
    
    For example, if there are two files for our sample contract `Z123456`, you should see:
    ```
    Z123456_0001.ndjson
    Z123456_0002.ndjson

### Cleanup After Scripts
Unless you create a new directory for each time the job is run, after all the files are downloaded, you should to cleanup the 
files created:
- Move the generated NDJSON files to another directory to be used as needed. If the job is re-run, the new files may interfere with the old files,
especially when the amount of data in the second job is less than the first. 
- Remove the generated files such as `jobId.txt` and `response.json`. The scripts should overwrite the data, but they are no longer
needed and can be deleted.

### Users with Multiple Contracts
If you have multiple contracts, you can search for both contracts at the same time but need to use different
directories. The files generated from the scripts such as `jobId.txt` and `response.json` relate to only one contract
so if you run them fom the same directory and at the same time, the scripts will overwrite the files.
Each contract has its own credentials so multiple text files can be created with the Base64 encoded
credentials and passed as a parameter. The actual data files downloaded (NDJSON files) contain the contract number in the file name
so they will not overwrite each other if a different contract is called. They will overwrite the files if the same job 
is run again and should be avoided because the result may have a different number of files can be confusing.

It is also important that you run the different jobs in different terminals so that each terminal will have their own
environment variables defined for their job.

## FAQs
Our [FAQ](https://docs.google.com/document/d/1INlu-QIsonE-KMjEW7R9W27MjRPPQRm9bzxn3y2rHFQ/edit?usp=sharing) document may answer additional questions you have. If not, refer to the Asking for Help section to get in touch. Thank you. 

