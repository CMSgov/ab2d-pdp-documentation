# Accessing AB2D Production Environment

## Audience

You are using this document because you wish to access your organization’s claims data from the production environment. At this point, your organization has attested, elected an ADOS, and has received credentials to access production. 

## Security Practices

The credential file provided during on-boarding contains sensitive information granting access to PII/PHI. 
Be extremely careful distributing this information and DO NOT SEND via email or chat as text, images, or videos.

The most sensitive information provided in the credential file includes:
- OKTA_CLIENT_ID: unique id of {organization} in the AB2D system
- OKTA_CLIENT_SECRET: unique password granting access to the AB2D system
- AUTH: basic encoding of the client id and password that can be easily decoded

## Asking for Help

If you continue to have issues accessing production or have follow-up questions not answered in this guide, please
email the AB2D team at [ab2d@cms.hhs.gov](mailto:ab2d@cms.hhs.gov) 
or join our Google Groups at: 
[https://groups.google.com/u/1/g/cms-ab2d-api](https://groups.google.com/u/1/g/cms-ab2d-api). 

When corresponding with the team please include the following information concerning the context of your issue:

- What operating system is in use by the machine (Windows or Linux/Mac)?
- What IP address does the machine have (see Verifying Setup)?
- If applicable, what HTTP status code (403, 400, etc.) is being received?
- A description of the issue including the stage of the export causing the issue.
- Any logs that may help us in resolving the issue. Use caution when sharing any log files as they may contain sensitive information. 
(Refer to the [Security Practices](#security-practices) section to identify sensitive information).

Do ***not*** include your organization's production credentials in any form when working with the AB2D team.
Please review all encoded content and/or logs before sharing with the team to ensure they do not contain your
credentials.

## Usage Guide

### API Restrictions:
Our export jobs are expensive to run and create files containing sensitive data. In order to mitigate these risks,
the API enforces usage restrictions and limits how long export files are stored by the system.

#### List of Restrictions: IMPORTANT
1.	Each contract is limited to one running export at a time. Jobs cannot be run concurrently. A job can be canceled if it is no longer needed or incorrectly submitted before submitting a new one.
2.	Files created by an export are only stored by the AB2D API for 72 hours after the job is complete. Files will be automatically deleted after 72 hours and a new job will have to be created.
3.	The API can only process so many export jobs at once. If the API is busy, a submitted job may be queued for later processing. 

What you can do:
- Start a single job, monitor the job, and download within the 72-hour period.
- Only run (1) job at a time. Attempting to start more than one job will fail.
- Cancel a running job whenever you want.
- Run jobs which generate either FHIR version STU3 (v1) or FHIR version R4 (v2) ExplanationOfBenefit objects

### AB2D API Versions

AB2D recommends using API version 2 ([FHIR R4](https://hl7.org/fhir/R4/)), which implements the [Bulk Data Access Implementation Guide V2.0.0](https://hl7.org/fhir/uv/bulkdata/). The `_until` parameter is only available with V2. [Learn more about migrating from V1 to V2](https://github.com/CMSgov/ab2d-pdp-documentation/raw/main/AB2D%20STU3-R4%20Migration%20Guide%20Final.xlsx).

The [HTTP query parameters](#http-query-parameters) section below is for V2 of the API. For organizations using V1 ([FHIR STU3](https://hl7.org/fhir/STU3/)), visit our [V1 documentation](./V1%20Long%20Term%20API%20Usage%20Model.md) to learn about parameters. 

- V2 (R4) - api.ab2d.cms.gov/api/v2/fhir
- V1 (STU3) - api.ab2d.cms.gov/api/v1/fhir

### Intended Usage / Suggestions

Parts A & B claims data are updated periodically. Pulling data every day or even weekly is unnecessary. Our advice is to pull data bi-weekly, monthly, or quarterly. Since export jobs can be time-consuming, especially for large organizations, scheduling them to run overnight is highly suggested.
***Be aware that files generated by an export job are only available for 72 hours after the job completes.*** [Learn more about the AB2D Incremental Export Model](https://github.com/CMSgov/ab2d-pdp-documentation/blob/main/docs/Long%20Term%20API%20Usage%20Model.md#incremental-export-model-overview).

### AB2D Model

***Explanation of Benefits (EOBs) claims are not exported by the date services were provided but by the date the EOB was fully processed into the CMS system.***
Conducting an export does not guarantee that you have received EOBs for all services provided up to that date, only EOBs processed by that date, as indicated by their updateDate.

##### Services Provided Date vs. Date Processed Scenario

We have two inpatient stays
1. Inpatient Stay A on October 1st, 2020
1. Inpatient Stay B on October 1st, 2020

For 1, an EOB claim is fully processed by CMS and available to the AB2D system on November 1st, 2020. If an organization exports claims data on November 5th, then claim A will be pulled.
For 2, an EOB claim is fully processed by CMS and available to the AB2D system on December 1st, 2020. If an organization exports claims data on November 5th, the claim for B will not be included. However, if the organization runs a later export on December 5th, the claim will be present and exported.
EOB data can be added and updated over a period of time. This can lead to scenarios where an organization exports an EOB twice. Organizations need to de-duplicate and update these EOBs.

Each EOB claim comes with a unique identifier and a last updated field. These fields can be used to conduct deduplication.

#### Claims Data Update Scenario
1.	Claim 123 is added for beneficiary on November 1st, 2020
2.	Organization XYZ pulls Claim 123 on November 2nd, 2020
3.	Claim 123 is updated on November 3rd, 2020
4.	Organization XYZ pulls the updated Claim 123 on November 4th, 2020

### HTTP Query Parameters

Query parameters allow users to filter claims data returned by date, which reduces duplication and speeds up job times. The parameter values follow the [ISO datetime format](https://en.wikipedia.org/wiki/ISO_8601) (`yyyy-MM-dd'T'hh:mm:ss[+|-]hh:mm`). The time zone must be specified using + or - followed by `hh:mm`. There are currently 2 optional parameters, `_since` and `_until`, which can be used separately or together: 

Separately, these parameters allow users to pull data last updated since **or** until a specified date. You can use the [meta/lastUpdated](https://ab2d.cms.gov/data_dictionary.html#:~:text=The%20date%20that%20this%20record%20was%20last%20updated%20within%20the%20database.) property of each ExplanationofBenefit (EOB) resource to find when each record was last updated. This will help you compare claims data when using the  `_since` and `_until` parameters. 
- For `_since`, the earliest possible date is February 13th, 2020 (`2020-02-13T00:00:00-05:00`) or your organization's attestation date, whichever is later. If no `_since` date is specified, it will default to the datetime of your organization’s last successfully and fully downloaded job. If this is your first job, it will default to your earliest possible date. 
- For `_until`, the latest possible date is the current date. If no `_until` date is specified or you use a date from the future, it will default to the current date.

Using the parameters together allows you to pull data last updated within a certain date range. However, the `_since` parameter value must be an earlier date than the `_until` parameter value. In other words, the `_until` datetime must have occurred after the `_since` datetime. [Learn more about how to use parameters](https://github.com/CMSgov/ab2d-pdp-documentation/blob/main/docs/Long%20Term%20API%20Usage%20Model.md).

### Expected Workflow

A job can be broken down into four phases. These phases will be standard no matter the programming language or platform used to interact with the AB2D API. Obtaining a token is done against a third party identification provider.

| Step | Goal | Swagger | Time | Frequency |
| --- | ---- | ----- | --- | --- |
| Obtain a JSON Web Token | Get a token for authentication with the AB2D API | N/A | Seconds | At least every hour during the job |
| Start a Job | Start a job and save the unique id of the job | Export API | Seconds | Once a job |
| Monitor a Job | Wait for the job to complete and get a list of files to download | Status API | Minutes to hours depending on contract size | Once every few minutes |
| Download the Files | Download the files listed in the returned data from the Status API from a complete job | Download API | Minutes to hours depending on Internet download speeds | Once a job

## Quickstart Step-by-Step Guides

The purpose of this document is to provide a guide for an organization’s ***first production run***. 
As part of this guide the AB2D team has built sample scripts demonstrating how to run and automate an export. 
These scripts are meant to help organizations get started with the AB2D API but are not meant for long term use in production as they do not provide sufficient error checking, security or auditing capabilities. 
Organizations should build more robust automation for their individual ETL processes.

Please focus your attention on ***Verifying Setup*** and ***Creating the Credentials File*** before attempting any of these scripts. Each individual guide assumes that the aforementioned steps have been completed.

Additionally, read and understand the usage of the Since Parameter as detailed in the Intended Usage / Suggestions section.

### Overview

1.  Requirements: information/resources required to export a job
1.	Verifying Setup: check that the machine used for exports has the appropriate access and that the credentials provided by the AB2D teamwork.
1.	AB2D Sandbox vs. Production differences that may influence the job.
1.	Creating credentials file: guide to creating a credentials file
1.	Executing a bulk claim data download:
      1. Bash guide (Linux & Mac)
      1. PowerShell Guide (Windows)
      1. Python Guide (Any environment but requires Python to be installed)

### Requirements

1.	The credentials file provided to your organization’s attestor
2.	Access to a machine with a allow-listed IP address
3.	Internet access for the allow-listed machine
4.	Permission on that machine to download files
5.	Permission on that machine to execute a Python script, Bash Terminal or PowerShell Terminal

### Verifying Your Setup
Confirm your machine’s access to the AB2D API before attempting to export data. 
During the on-boarding process, your organization provided 8 IPs to be allow-listed by the AB2D team.
You will need to confirm that the IP address of the machine performing the export matches one of these allow-listed IP addresses.
Remember that these IP addresses must be public and static. Please email the AB2D team if any changes are needed, and we will provide you with the proper form. 

#### Verifying Your IP Address
To check your IP:
1. If you have a browser, open it on the machine you are connecting from, and go to this website 
[http://checkip.amazonaws.com/](http://checkip.amazonaws.com/).
Remember your IP should be public and static. Your Organization should have provided no more than 8 IPs to be allow-listed. Should you need to change these, please email us and we will provide you with a form to do so. 
2. If you do not have access to a browser on the connecting machine, query from the command line of the machine you are running on.
    1. On Linux/Mac open a terminal and run the following command: “curl -X GET checkip.amazonaws.com”
    2. On Windows open a Powershell terminal and run the following command “Invoke-RestMethod -Method GET checkip.amazonaws.com”

#### Verify AB2D Production Connectivity
Check that the machine performing the export belongs to the provided IPs.

On the command line run one of the two following commands:

- On Linux/Mac in a terminal run:
``` curl -X GET https://api.ab2d.cms.gov/health --verbose ```

- On Windows in a powershell terminal run 
```Invoke-RestMethod -Method GET https://api.ab2d.cms.gov/health```

- In Postman type create a new GET request against the following ```URL: https://api.ab2d.cms.gov/health```.

- In a browser, on the machine, go to the following URL ```https://api.ab2d.cms.gov/swagger-ui/index.html```

If the response has an HTTP status of 200 then the request is okay and your IP address is allow-listed.

If the response has an HTTP status of 403 your URL request has been rejected and your IP address is possibly not allow-listed.

### AB2D Sandbox vs. AB2D Production

- ***Sandbox:*** The sandbox AB2D environment is a public environment available for users to learn how the API works and 
conduct testing for newly developed features, tools and automation using the API.
- ***Production:*** The production AB2D environment is a private environment available only to attested and credentialed PDPs for exporting relevant patient data.

Key differences:
- Contracts/Data - Sandbox contains test contracts with synthetic data which are publicly available
- URL - The sandbox and production environments have different URLs to access them:
  - Sandbox - ```https://sandbox.ab2d.cms.gov```
  - Production - ```https://api.ab2d.cms.gov```
- Identity provider - the sandbox identity provider is a test provider and cannot provide a bearer token giving you access to real claims data. Only the production identity provider can give access to your organization’s claims data. The locations from which you can retrieve bearer tokens differ: 
  - Sandbox - ```https://test.idm.idp.cms.gov```
  - Production - ```https://idp.cms.gov```
- IP Allow-listed - our production environment provides extremely limited access. Only IP addresses explicitly provided by your organization can access the production system.

### Create Base64 Credentials Store

[Base64](https://en.wikipedia.org/wiki/Base64) is an encoding type that allows binary data to be encoded into text values that work better with
systems that only reliably support text content. It does not encrypt data, it simply encodes it so that it is easier to transmit.
Because passwords are often unpredictable, most web based authentication requires Base64 encoded credentials. This is the case
with our API. This is also why you must keep both the credentials and the encoded values secret. To receive a JSON Web Token
from Okta to pass to our API, we must pass a Base64 encoded clientId:password to the Basic Authorization header.
You can either create these manually from your credentials or use the one we provided for convenience.
We will be putting this encoded value in a file so that our scripts can use it for authentication. Name the file after
the contract number since authorization is done at the contract level. For this example, let's say the contract
number is `Z123456`, our client ID is `abcd` and password is `badpassword`. If we encoded our example client ID and 
password, it would look like `YWJjZDpiYWRwYXNzd29yZA==`. Your credentials will likely be a little longer, but that
will give you an idea of what the value looks like.

#### Use the One Supplied in Credential File

For your convenience, we have provided the Base64 encoded clientId:password. 

1. Open the credentials file provided by the AB2D team
2. Copy the Base64 encoded string inside the credential file
3. Create a new text file using your editor of choice
4. Paste the Base64 encoded string into the new text file (the file should only have one line and look similar to our example)
5. Save that file as the credential file (for example `C:\users\abcduser\credentials_Z123456_base64.txt`) and note 
the location and name of the file for later as it will be a parameter in other scripts.

#### Manually create credential file
If you wish to manually create this file from your credentials, you can:

#### In Linux or Mac:
You will be creating a file in a directory to place the Base64 credentials. Make sure you have write access to that
directory. Let's use `credentials_Z123456_base64.txt` in the user's home directory of `/home/abcduser` as an example.

- Open a bash terminal and type:

```
AUTH_FILE=/home/abcduser/credentials_Z123456_base64.txt
OKTA_CLIENT_ID=abcd
OKTA_CLIENT_SECRET=badpassword
```
- To encode the credentials as base64
```
echo -n "${OKTA_CLIENT_ID}:${OKTA_CLIENT_PASSWORD}" | base64 > $AUTH_FILE
```

#### In Powershell 

- Open a PowerShell terminal

- Create a new empty file:

```
$AUTH_FILE=C:\users\abcduser\credentials_Z123456_base64.txt
New-Item -Path $AUTH_FILE -ItemType File
```

- Create the base64 credentials:
```
$BASE64_ENCODED_ID_PASSWORD='{Base64-encoded abcd:badpassword}'
```
- Save the base64 credentials as the only line in the auth file:
```
Set-Content -Path $AUTH_FILE $BASE64_ENCODED_ID_PASSWORD
```

The credential file should contain one line and contain the Base64 encoded value of the client ID and password.

## API Guides

This section provides step-by-step instructions for the use of sample Bash, Powershell and Python scripts built 
by the AB2D team to demonstrate how to run and automate an export. These clients are provided as examples and 
are not meant for long term use in production. 

This may be a great starting point for your engineering or development teams however it is important to note 
that the AB2D team does not regularly maintain the sample clients. Additionally, a best-effort was made to 
ensure the clients are secure, but they have not undergone comprehensive formal security testing. Each user/organization 
is responsible for conducting their own review and testing prior to implementation.

When used in production, these clients have the ability to download PII/PHI information. You should therefore 
ensure the environment in which these scripts are run is secured in a way to allow for storage of PII/PHI. 
Additionally, when used in the production environment the scripts will require use of your production credentials. 
Please ensure that your credentials are handled in a secure manner and not printed to logs or the terminal. 
Ensuring the privacy of data is the responsibility of each user and/or organization.

### Sample Bash API Client Step-by-Step Guide

This guide shows how to use our sample Bash API client consisting of several scripts. The example API client is 
free to use or extend. However, this client is just an example implementation and organizations should implement their
own client suited to their needs.

| Scripts| |
| :---------------------------------  | --- |
| `bootstrap.sh`        | set up environment variables necessary for other scripts |
| `start-job.sh`        | start a job and save the job id for future use |
| `monitor-job.sh`      | monitor a running job using the saved job id and save list of files to download when the job completes|
| `download-results.sh` | download the created files |

#### Requirements
- Bash shell on the machine with the white listed IP address
- Contract number if there are more than one
- Directory - the directory that you wish to save all the files in
- Auth File - location of the file containing the organization’s base64 encoded credentials
- Since date - if using the v1/STU3 version of the API and only want claim data entered or updated in the system after a certain 
date or do not want to use the default date provided by v2/R4 of the API, you must provide a `_since` date matching
the format: `2020-05-01T00:00:00.000-05:00`
- Enough space on the file system for the exported claims data 

##### Download the Scripts
1. The bash API is available [here](https://github.com/CMSgov/ab2d-sample-client-bash). Either download a ZIP file of
the repository or clone the repository using these [instructions](https://docs.github.com/en/free-pro-team@latest/github/creating-cloning-and-archiving-repositories/cloning-a-repository).

2. Unzip or move the files into a specified directory. For our example, let's create a directory called `ab2d` in the home directory.
Copy those script files to that directory (`/home/abcduser/ab2d`).

##### Run the Scripts

###### Set Up the Environment
1. Open a bash shell. Use this same shell to run all commands. We will be setting environment variables that are only
valid inside this shell. ***Do not close this terminal before the download is complete***.
2. Go to the directory where the script files are located (in our example `/home/abcduser/ab2d`). Perform a `ls` in that directory to make sure the four scripts are there.
3. Remember where you put the Base64 credential file (in our example, it was `/home/abcduser/credentials_Z123456_base64.txt`).
4. Run the following command:
```
source bootstrap.sh -prod --directory /home/abcduser/ab2d --auth /home/abcduser/credentials_Z123456_base64.txt --since 2020-05-01T00:00:00.000-05:00 --fhir R4
```
Verify that the command worked and defined the correct environment variables:
```
echo $DIRECTORY
/home/abcduser/ab2d

echo $IDP_URL
https://idm.cms.gov/oauth2/aus2ytanytjdaF9cr297/v1/token

echo $API_URL
https://api.ab2d.cms.gov/api/v2/fhir

echo $AUTH_FILE
/home/abcduser/credentials_Z123456_base64.txt

echo $SINCE
2020-05-01T00:00:00.000-05:00

echo $FHIR_VERSION
R4
```

###### Start a Job
This script will use the $export endpoint to start a job. From the same window used to create the environment variables and in the same directory where the scripts are located:
```
./start-job.sh
```
Verify that a file called `jobId.txt` was created. 

###### Monitoring a Job
Because this job is asynchronous, you must check on the status of the job periodically until the job is complete.
When a job completes, the response to the status check will be the created files. This script will 
continue to check the status until it is complete. You do not have to run this script more than once (unless you exit it)
as it will pause and recheck. Use the same shell.

```
./monitor-job.sh
```

Once this script exists, it should have created a file called `response.json` in the current directory. 
This file will contain all the names of the files created as part of this batch job. Files have a max size, 
so if the number of claim data records causes a file to exceed that size, a new file is created.
 
###### Download the Claims Data Files
Once `monitor-job.sh` finishes, we can now download the files from AB2D (again from the same shell).

```
./download-results.sh
```
The files will be NDJSON (New line delimited JSON) files. That means each line in the files is one claim written in JSON.
The naming standard for the files is to use the contract number and then a number indicating which value it is in the series
of created files.

In the directory you specified as the parameter to the `bootstrap.sh` command (in our case 
`/home/abcdhome/ab2d`), you can see the files. If there are two files for our sample contract `Z123456`, you should see:
```
ls /home/abcdhome/ab2d/*.ndjson
Z123456_0001.ndjson
Z123456_0002.ndjson
```
### Sample Windows API Client Step-by-Step Guide
The sample Windows API Client will always download files to your current working directory. This means that 
if you run a job you should move the files out of the directory afterwards into a new directory 
just for that download.

#### Requirements
- Powershell on the machine with the white listed IP address
- Contract number if there are more than one
- Directory - the directory that you wish to save all the files in
- Auth File - location of the file containing the organization’s base64 encoded credentials
- Since date - if using the v1/STU3 version of the API and only want claims data entered or updated in the system after a 
certain date or do not want to use the default date provided by v2/R4 of the API, you must provide a `_since` date matching the 
following format: 2020-05-01T00:00:00.000-05:00
- Enough space on the file system for the exported claims data 

#### Download API Client
1. Download the bash api client available [here](https://github.com/CMSgov/ab2d-sample-client-powershell). 
Either download a ZIP file of the repository or clone the repository using these [instructions](https://docs.github.com/en/free-pro-team@latest/github/creating-cloning-and-archiving-repositories/cloning-a-repository).
2. Open powershell terminal and navigate to the home directory of the project
3. Run the `dir` command. You should see two powershell scripts (.ps1) and a `README`

#### Prepare the Environment Variables
1. Open a PowerShell terminal and go do the directory which contains the powershell scripts.
2. Set the location of the file containing your organizations AB2D credentials encoded as Base64
    ```
    $AUTH_FILE=C:\users\abcduser\credentials_Z123456_base64.txt
    ```

3. Set the URL that the authentication provider is available at 
    ```
    $AUTHENTICATION_URL='https://idm.cms.gov/oauth2/aus2ytanytjdaF9cr297/v1/token'
    ```

4. Set the URL that AB2D is available at (v2 for FHIR R4 and v1 for FHIR STU3)
    ```
    $AB2D_API_URL='https://api.ab2d.cms.gov/api/v2'
    ```

#### Start and Monitor a Job
This script will use the environment defined above, create a new job and monitor it
until it is complete. Once complete, it outputs a list of files to be downloaded.

1. In the same PowerShell terminal start and monitor the job:
    ```
    $JOB_RESULTS = &.\create-and-monitor-export-job.ps1 | select -Last 1
    ```
2. Check the contents of the variable `JOB_RESULTS`. It should contain the list of files to download.
    ```
    $JOB_RESULTS to check the contents of the variable
    ```
    Do not close the shell so that the value of `JOB_RESULTS` is preserved.
    
#### Download file(s)
Next, we download the files specified in `JOB_RESULTS` into the current directory.

In the same PowerShell terminal download the results.

```
.\download-results.ps1
```
    
Check whether the files were downloaded.

```
dir
```
   
The files will be NDJSON (New line delimited JSON) files. That means each line in the files is one claim written in JSON.
The naming standard for the files is to use the contract number and then a number indicating which value it is in the series
of created files.
       
For example, if there are two files for our sample contract `Z123456`, you should see:
```
Z123456_0001.ndjson
Z123456_0002.ndjson
```
   
### Sample Python API Client Step-by-Step Guide
The sample Python API Client can be used in any environment, but must be installed on the machine with the white listed IP address.
To install Python3 and Pip (Python Installation Installer), go [here](https://realpython.com/installing-python/).
Make sure you have at least Python 3.6 and pip3 installed. Some machines come with Python2 by default which
is deprecated.

#### Requirements
Bash or PowerShell
- Python3 and Pip installed
- Contract number if there are more than one
- Directory - the directory that you wish to save all the files in
- Auth File - location of the file containing the organization’s base64 encoded credentials
- Since date - if using the v1/STU3 version of the API and only want claims data entered or updated in the system after a 
certain date or do not want to use the default date provided by v2/R4 of the API, you must provide a `_since` date matching 
the following format: 2020-05-01T00:00:00.000-05:00
- Enough space on the file system for the exported claims data 

#### Download API Client
1. Download the bash api client available [here](https://github.com/CMSgov/ab2d-sample-client-python). 
Either download a ZIP file of the repository or clone the repository using these [instructions](https://docs.github.com/en/free-pro-team@latest/github/creating-cloning-and-archiving-repositories/cloning-a-repository).
2. Open a terminal and navigate to the home directory of the downloaded files. Let's say it is either
`C:\Users\abcduser\ab2d\` for windows or `/home/abcduser/ab2d` for linux/Mac. Verify the files
exist by either doing a `dir` or `ls`. You should see the `job-cli.py` script and a `README`.

#### Prepare Environment Variables

1. In a Bash Shell or PowerShell terminal, navigate to the directory with the script file.
2. Set the variable `AUTH_FILE` which points to an existing file containing the base64 credentials.

    | Environment | Command |
    | --- | --- |
    | Linux/Mac: | `AUTH_FILE=/home/abcduser/credentials_Z123456_base64.txt` |
    | Windows: | `$AUTH_FILE=C:\users\abcduser\credentials_Z123456_base64.txt` |

3. Define a directory to save the exported files in. Create it if necessary. This is where the export files
will be saved. For our example, let's say it's `/home/abcduser/ab2d` or `C:\users\abcduser\ab2d`.
4. Set the variable `DIRECTORY` which points to that directory in the previous step.

    | Environment | Command |
    | --- | --- |
    | Linux/Mac: | `DIRECTORY=/home/abcduser/ab2d` |
    | Windows: | `$DIRECTORY=C:\users\abcduser\ab2d` |
    
5. Do not close this shell.

#### Start the Job
Use the same shell used to prepare the environment variables to perform this task. 
1. Make sure the `python` command is mapped to the correct version by performing the following command:
    ```
    python --version
    ```
   You should see a version that is at least 3.6.
2. Start the export job:

    | Environment | Command |
    | --- | --- |
    | Linux/Mac: | `python job-cli.py -prod --auth $AUTH_FILE --directory $DIRECTORY --since ‘2020-05-01T00:00:00.000-05:00’ --fhir R4 --only_start` |
    | Windows: | `python job-cli.py -prod --auth %AUTH_FILE% --directory %DIRECTORY% --since ‘2020-05-01T00:00:00.000-05:00’ --fhir R4 --only_start` |
    
3. Verify that a job id was created. The `job_id.txt` file should have been created. The file should contain
a string similar in ***format*** to `133039b8-c74c-422f-8836-8335c13f5a8d`.

    | Environment | Command |
    | --- | --- |
    | Linux/Mac: | `cat $DIRECTORY/job_id.txt` |
    | Windows: | `type %DIRECTORY%\job_id.txt` |

#### Monitor the Job
A job can take anywhere from 2 minutes to several hours. If the monitoring script fails for any
reason, restart the script. There are no side effects from monitoring the job so this script can be run as many times as necessary.
Use the same shell used to prepare the environment variables to perform this task. 

1. Begin monitoring the job

    | Environment | Command |
    | --- | --- |
    | Linux/Mac: | `python job-cli.py -prod --auth $AUTH --directory $DIRECTORY --only_monitor` |
    | Windows: | `python job-cli.py -prod --auth %AUTH% --directory %DIRECTORY% --only_monitor` |
    
2. When the job is completed the script should automatically save a list of files to download. To view the list:

    | Environment | Command |
    | --- | --- |
    | Linux/Mac: | `cat $DIRECTORY/response.json` |
    | Windows: | `type %DIRECTORY%\response.json` |

#### Download the Files

This process will only download the files. Running again will overwrite the files. 

1. In the same shell used to prepare the environment variables, download the files:

    | Environment | Command |
    | --- | --- |
    | Linux/Mac: | `python job-cli.py -prod --auth $AUTH --directory $DIRECTORY --only_download` |
    | Windows: | `python job-cli.py -prod --auth %AUTH% --directory %DIRECTORY% --only_download` |

2. List the files that you have downloaded and verify that they are present

    | Environment | Command |
    | --- | --- |
    | Linux/Mac: | `ls $DIRECTORY/*.ndjson` |
    | Windows: | `dir %TARGET_DIR%\*.ndjson` |
    
    The files will be NDJSON (New line delimited JSON) files. That means each line in the files is one claim written in JSON.
    The naming standard for the files is to use the contract number and then a number indicating which value it is in the series
    of created files.
    
    For example, if there are two files for our sample contract `Z123456`, you should see:
    ```
    Z123456_0001.ndjson
    Z123456_0002.ndjson

### Cleanup After Scripts
Unless you create a new directory for each time the job is run, after all the files are downloaded, you should to clean up the 
files created:
- Move the generated NDJSON files to another directory to be used as needed. If the job is re-run, the new files may interfere with the old files,
especially when the amount of data in the second job is less than the first. 
- Remove the generated files such as `jobId.txt` and `response.json`. The scripts should overwrite the data, but they are no longer
needed and can be deleted.

### Users with Multiple Contracts
If you have multiple contracts, you can search for both contracts at the same time but need to use different
directories with different credentials. The files generated from the scripts such as `jobId.txt` and `response.json` relate to only one contract
so if you run them from the same directory and at the same time, the scripts will overwrite the files.
Each contract has its own credentials so multiple text files can be created with the Base64 encoded
credentials and passed as a parameter. The actual data files downloaded (NDJSON files) contain the contract number in the file name,
so they will not overwrite each other if a different contract is called. They will overwrite the files if the same job 
is run again and should be avoided because the result may have a different number of files and can be confusing.

It is also important that you run the different jobs in different terminals so that each terminal will have their own
environment variables defined for their job.

## FAQs
Our [FAQ](./Production%20User%20FAQs.md) document may answer additional questions you have. If not, refer to the Asking for Help section to get in touch. Thank you. 

